{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics of Stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, re, pprint, os, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "pltsettings = {\n",
    "    \"figure.figsize\" : (5.0, 4.0),\n",
    "    \"pgf.texsystem\" : \"pdflatex\",\n",
    "    \"font.family\": \"sans\",\n",
    "    \"font.serif\": [],                   # use latex default serif font\n",
    "    #\"font.sans-serif\": [\"DejaVu Sans\"], # use a specific sans-serif font\n",
    "}\n",
    "matplotlib.rcParams.update(pltsettings)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns # makes exports ugly\n",
    "\n",
    "print plt.style.available\n",
    "plt.style.use ('seaborn-whitegrid')\n",
    "\n",
    "Pantone540=(0,.2,0.34901)\n",
    "Pantone301=(0,0.32156,0.57647)\n",
    "TUMred=(0.898,0.2039,0.0941)\n",
    "TUMgreen=(0.5686,0.6745,0.4196)\n",
    "TUMgray=(0.61176,0.61568,0.62352)\n",
    "TUMlightgray=(0.85098,0.85490,0.85882)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get path to files\n",
    "print datetime.datetime.now()\n",
    "unitstats_file = os.environ.get('unitstats', '/tmp/unitstats.log')\n",
    "\n",
    "unitstats_file=\"/home/becker/async/StratoX.git.rcs/obj/unitstats.log\"\n",
    "if not os.path.exists(unitstats_file):\n",
    "    print(\"ERROR: File {} does not exist.\".format(unitstats_file))\n",
    "    exit(-1)\n",
    "FOLDER=os.path.split(unitstats_file)[0]\n",
    "print \"file=\" + str(unitstats_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_json(fname):\n",
    "    \"\"\"\n",
    "    reads JSOn and returns (totals, units), tuple of dicts\n",
    "    \"\"\"\n",
    "    units=None\n",
    "    totals=None\n",
    "    with open(fname,'r') as f:\n",
    "        endofunits = False\n",
    "        for line in f:\n",
    "            match = re.search(r'^TOTALS', line)        \n",
    "            if match:                \n",
    "                endofunits = True\n",
    "            if not endofunits:\n",
    "                try:\n",
    "                    if not units:\n",
    "                        units = json.loads(line)\n",
    "                    else:\n",
    "                        print \"error: units appearing multiple times\"\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                try:\n",
    "                    if not totals:\n",
    "                        totals = json.loads(line)\n",
    "                    else:\n",
    "                        print \"error: totals appearing multiple times\"\n",
    "                except:\n",
    "                    pass\n",
    "    #print units\n",
    "    # unpack units (list of dicts) => dict\n",
    "    tmp=units\n",
    "    units={}\n",
    "    for u in tmp:\n",
    "        name=u.keys()[0]\n",
    "        stats=u[name]        \n",
    "        units[name]=stats       \n",
    "    return (totals, units)\n",
    "            \n",
    "#######\n",
    "(totals, units) = read_json(unitstats_file)\n",
    "print \"TOTALS: \"\n",
    "pprint.pprint(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### plot success per VC type; rows are VCs, columns are \"success\", \"fail\"\n",
    "def pretty_vcname(VCname):\n",
    "    parts = VCname.split(\"_\");\n",
    "    if parts[0] == \"VC\":\n",
    "        parts = parts[1:]    \n",
    "    return \" \".join(parts).title()\n",
    "\n",
    "counters={pretty_vcname(k) : v['cnt'] for k,v in totals[\"rules\"].iteritems()}\n",
    "df_cnt = pd.DataFrame(counters,index=['count']).T\n",
    "#print df2.head()\n",
    "# sort by occurence\n",
    "df_cnt.sort_values(by='count', ascending=False, inplace=True)\n",
    "ax=df_cnt.plot.bar(color=[Pantone301],legend=False);\n",
    "ax.set_ylabel('count')\n",
    "#ax.grid()\n",
    "plt.savefig(FOLDER + os.sep + 'props.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "num_vc = sum([v for k,v in counters.iteritems() if k != 'Uninitialized'])\n",
    "print \"########################\"    \n",
    "print \"# Percentage of each VC\"\n",
    "print \"########################\"\n",
    "for k,v in counters.iteritems():\n",
    "    if k != 'Uninitialized': print k + \": {0:0.1f}\".format(100.0*v/num_vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Ratio of Properties (Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot success per VC type; rows are VCs, columns are \"success\", \"fail\"\n",
    "VCs={pretty_vcname(k) : {'proven' : 100.0*v['proven'] / v['cnt'], 'unsuccessful': 100.0*(v['cnt']-v['proven'])/v['cnt'] } for k,v in totals[\"rules\"].iteritems()}\n",
    "df2 = pd.DataFrame(VCs).T\n",
    "#print df2.head()\n",
    "df2.sort_values(by='proven', inplace=True, ascending=False)\n",
    "exclude_columns=['unsuccessful']\n",
    "ax=df2.ix[:,df2.columns.difference(exclude_columns)].plot.bar(stacked=True,color=[Pantone301],legend=False,figsize=(5,2.5));\n",
    "ax.set_ylim(50,100)\n",
    "ax.set_ylabel('success [%]')\n",
    "ax.set_title('Verification success per VC type')\n",
    "\n",
    "plt.savefig(FOLDER + os.sep + 'vc_success.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Ratio by unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unit_success = {k : {'success': v['success'], 'cnt' : v['props']} for k,v in units.iteritems()}\n",
    "df_usucc = pd.DataFrame(unit_success).T\n",
    "df_usucc.sort_values(by='success', inplace=True, ascending=False)\n",
    "exclude_columns=['cnt']\n",
    "ax=df_usucc.ix[df_usucc.cnt>0,df_usucc.columns.difference(exclude_columns)].plot.bar(color=[Pantone301],legend=False);\n",
    "ax.set_ylabel('success')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unit_cnt = {k : { 'proven' : v['proven'], 'unsuccessful' : v['props'] - v['proven'], 'cnt':v['props'], 'success' : 100.0 if v['props']==0 else (100.0*v['proven'])/v['props']} for k,v in units.iteritems() if v['props']>0}\n",
    "df_ucnt = pd.DataFrame(unit_cnt).T\n",
    "# filter those where cnt=0\n",
    "\n",
    "\n",
    "print df_ucnt.head()\n",
    "df_ucnt.sort_values(by=['cnt','proven'], inplace=True, ascending=False)\n",
    "exclude_columns=['cnt','success']\n",
    "\n",
    "ax=df_ucnt.ix[df_ucnt.cnt>0,df_ucnt.columns.difference(exclude_columns)].plot.bar(stacked=True,color=[TUMgreen,TUMred],figsize=(7,4));              \n",
    "ax.set_ylabel('cnt')\n",
    "ax.set_xlabel('unit')\n",
    "#ax.grid()\n",
    "plt.savefig(FOLDER + os.sep + 'units_props.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure Ratio does not depend on #props:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ucnt.plot.scatter(x='cnt', y='success');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage per Unit\n",
    "Show that HAL/BSP are just spec and app is full SPARK\n",
    "TODO: Units which have SPARK-Mode off in spec -> they do not appear here, which is unfair\n",
    "\n",
    "### SPARK_Mode\n",
    "Tri-state:\n",
    " * On (Must be in SPARK)\n",
    " * Off (forbid GNATprove to analyze this)\n",
    " * Auto (implicit: take whatever is compliant to SPARK as SPARK; ignore rest)\n",
    "\n",
    "### What does \"spec\" mean?\n",
    "Does it mean that there is a contract, or only that the spec is in a scope with SPARK mode on?\n",
    "\n",
    "### Mixed Unit\n",
    "Does it count non-SPARK subs in the body?\n",
    "\n",
    "### Non-SPARK Unit\n",
    "Does GNATprove count the specs even? \n",
    " * yes, but only SPARK-compliant specs. E.g., it skips functions with side effects   \n",
    " * FAT_Filesystem.Directories: lot of functions. Ents=0\n",
    " \n",
    "We need to count entities ourselves, somehow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter=['stm32','cortex','generic']#,'hil.','driver','hal','register']\n",
    "unit_cov = {u : {'body': v[\"coverage\"], 'spec': 100.0*v['spec'] / v['ents'] if v['ents'] > 0 else 0, 'ents' : v['ents'], 'skip': 100.0*v['skip']/v['ents']} for u,v in units.iteritems() if (not any(f in u for f in filter) and v['ents']>0)}\n",
    "df_ucov = pd.DataFrame(unit_cov).T\n",
    "df_ucov.sort_values(by=['body','spec','skip'], inplace=True, ascending=False)\n",
    "exclude_columns=['ents']\n",
    "\n",
    "ax=df_ucov.ix[:,df_ucov.columns.difference(exclude_columns)].plot.bar(stacked=True,figsize=(13,3),color=[TUMgreen, \"black\", TUMlightgray]);\n",
    "ax.set_ylabel('coverage')\n",
    "plt.savefig(FOLDER + os.sep + 'units_cov.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "print units[\"fat_filesystem.directories\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flows by unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter=['stm32','cortex','generic']\n",
    "unit_flow = {u : {'cnt': v['flows'], 'success': v['flows_success'] } for u,v in units.iteritems() if (not any(f in u for f in filter) and v['flows']>0)}\n",
    "df_flows = pd.DataFrame(unit_flow).T\n",
    "df_flows.sort_values(by=['cnt'], inplace=True, ascending=False)\n",
    "exclude_columns=[]\n",
    "ax=df_flows.ix[:,df_flows.columns.difference(exclude_columns)].plot.bar();\n",
    "ax.set_ylabel('success')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by task? not yet possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# specific check by package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_vc (vcname):\n",
    "    \"\"\"\n",
    "    Plot success for a specific VC in all units\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for u,v in units.iteritems():\n",
    "        for r, rv in v['rules'].iteritems():\n",
    "            if r == vcname and rv and 'cnt' in rv and rv['cnt'] > 0:\n",
    "                res[u] = { 'cnt' : rv['cnt'], 'proven' : rv['proven'], 'fail' : rv['cnt'] - rv['proven'] }\n",
    "\n",
    "    df_vc = pd.DataFrame(res).T\n",
    "    print df_vc.head()\n",
    "    df_vc.sort_values(by=['cnt','proven'], inplace=True, ascending=False)\n",
    "    exclude_columns=['cnt']\n",
    "\n",
    "    ax=df_vc.ix[:,df_vc.columns.difference(exclude_columns)].plot.bar(stacked=True,figsize=(13,7),color=[TUMred, TUMgreen]);\n",
    "    ax.set_ylabel('num')\n",
    "    ax.set_title('Results for ' + pretty_vcname(vcname))\n",
    "    #ax.grid()\n",
    "    plt.savefig(FOLDER + os.sep + 'units' + vcname + '.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(totals[\"rules\"].keys()[0])\n",
    "for vc in totals[\"rules\"].keys():\n",
    "    plot_vc(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis Time per Module and VC Type (only SPARK 17+)\n",
    "Because the entry \"check_tree\" is only available in GNATprove 2017 and later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print str(units['mission']['details_proofs'])\n",
    "# 'check_tree' : [{'proof_attempts':{ <solver> : {'steps:..,'result':..,'time':..}}, 'transformations': {}}]\n",
    "\n",
    "def parse_checktree(checktree):\n",
    "    \"\"\"\n",
    "    decend into check_tree and return:\n",
    "     - time sum of all proof attempts\n",
    "     - count how often each solver was used\n",
    "     - find most steps needed\n",
    "    \"\"\"\n",
    "    ret = {'time': 0.0, 'solvers': {}, 'maxsteps': 0, 'result' : 'undefined'}\n",
    "    howproved_counted = ['interval','flow']\n",
    "    if not checktree:         \n",
    "        return ret\n",
    "    \n",
    "    def strip_result(r):\n",
    "        \"\"\"\n",
    "        Remove additional information from result and return a result class in {Valid, ...}\n",
    "        \"\"\"\n",
    "        # somehow step limits are reported not as \"Step limit exceeded\"; but often as \"Failure (steps:<n>)\"\n",
    "        # we remap them to the expected strings\n",
    "        match = re.search(r'\\w+ \\(([^\\)]+)\\)', r)\n",
    "        if match:            \n",
    "            detail = match.group(1)\n",
    "            if 'steps' in detail:\n",
    "                return 'Step limit exceeded'\n",
    "            elif 'time' in detail: # never seen this\n",
    "                return 'Timeout'\n",
    "            elif 'resource limit' in detail: # this must be why3-cpulimit. Either timeout or memout.\n",
    "                return 'Resource limit exceeded'\n",
    "            else:\n",
    "                return detail\n",
    "        else:\n",
    "            return r\n",
    "    \n",
    "    result = set()\n",
    "    for chk in checktree:\n",
    "            #print \"chk=\" + str(chk)\n",
    "            if 'transformations' in chk and len (chk['transformations']) > 0:\n",
    "                print \"trafo; not handled: \" + str(chk)\n",
    "                return None\n",
    "            if 'proof_attempts' in chk:\n",
    "                for s,sd in chk['proof_attempts'].iteritems():\n",
    "                    #print \"attempt=\" + str(s) + \":\" + str(sd)\n",
    "                    if not s in ret['solvers']: ret['solvers'][s] = {}\n",
    "                    ret['solvers'][s]['cnt'] = ret['solvers'][s].get('cnt', 0) + 1\n",
    "                    ret['time'] += sd['time']                    \n",
    "                    if sd['steps'] > ret['maxsteps']: ret['maxsteps'] = sd['steps']\n",
    "                    result.add(strip_result(sd['result']))                        \n",
    "            else:\n",
    "                print \"no proof attempts\"\n",
    "            if 'how_proved' in chk and chk['how_proved'] in howproved_counted:\n",
    "                ret['solvers'][chk['how_proved']] = ret['solvers'].get(chk['how_proved'], 0) + 1    \n",
    "    \n",
    "    # decide overall result of all provers    \n",
    "    #print \"result=\" + str(result)\n",
    "    if len(result) == 1:        \n",
    "        ret[\"result\"] = next(iter(result))        \n",
    "    else:\n",
    "        # multiple different reasons: if there is a timeout/stepout, then give this as reason; otherwise mention what is there        \n",
    "        if \"Valid\" in result:\n",
    "            ret[\"result\"] = 'Valid'\n",
    "        elif any (substring in result for substring in [\"Timeout\", \"Step limit exceeded\", \"Resource limit exceeded\", \"Out Of Memory\"]):\n",
    "            reason = []\n",
    "            if \"Step limit exceeded\" in result: reason.append(\"steplimit\")\n",
    "            if \"Of Of Memory\" in result: reason.append(\"memlimit\")\n",
    "            if \"Resource limit exceeded\" in result: reason.append(\"resourcelimit\")\n",
    "            if \"Timeout\" in result: reason.append(\"timeout\")\n",
    "            ret[\"result\"] = '|'.join(list(reason))\n",
    "        else:            \n",
    "            ret[\"result\"] = '|'.join(list(result))\n",
    "    #print \"consolidated result:\" + ret[\"result\"]\n",
    "    return ret\n",
    "\n",
    "VCs = totals[\"rules\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# summarize per VC\n",
    "####################\n",
    "res = {}\n",
    "have_checktree = False\n",
    "for u,ud in units.iteritems():\n",
    "    res[u] =  { 'VC': { v : {'cnt':0, 'solvers': {}, 'time_total': 0.0, 'time_total_undecided' : 0.0, 'time_samples' :[], 'time_samples_undecided' : [], 'result': {}} for v in VCs }}\n",
    "    t_total = 0.0\n",
    "    t_total_undecided = 0.0\n",
    "    maxsteps = 0\n",
    "    if \"details_proofs\" in ud:        \n",
    "        #print \"num proofs in \" + u + \"=\" + str(len(ud['details_proofs']))\n",
    "        for p in ud['details_proofs']:\n",
    "            vc = p['rule']\n",
    "            res[u]['VC'][vc]['cnt'] += 1\n",
    "            ct = None\n",
    "            if 'check_tree' in p: \n",
    "                ct = parse_checktree(p['check_tree'])            \n",
    "            if ct:\n",
    "                have_checktree = True\n",
    "                t_total += ct['time']\n",
    "                res[u]['VC'][vc]['time_total'] += ct['time']                  \n",
    "                res[u]['VC'][vc]['time_samples'].append (ct['time'])\n",
    "                if ct['maxsteps'] > maxsteps: maxsteps = ct['maxsteps']\n",
    "                # merge dicts counting solver invocations\n",
    "                for k,v in ct['solvers'].iteritems():\n",
    "                    res[u]['VC'][vc]['solvers'][k] = res[u]['VC'][vc]['solvers'].get(k,0) + v['cnt']\n",
    "                # we get no result class when there is no check tree. this happens for interval\n",
    "                if ct['result'] == 'undefined':\n",
    "                    if p['how_proved'] == 'interval':\n",
    "                        ct['result'] = 'Valid' if p['severity'] == 'info' else 'interval failed'\n",
    "                    else:\n",
    "                        print \"ERROR: unhandled value of how_proved. Not a solver, not interval check. what is it?\"\n",
    "                # we get back exactly one result class per check. Count how often this happens in the unit\n",
    "                res[u]['VC'][vc]['result'][ct['result']] = res[u]['VC'][vc]['result'].get(ct['result'],0) + 1\n",
    "                # now also collect samples and sum time for proofs that did not finish\n",
    "                if not ct['result'] in [\"Valid\", \"HighFailure\", \"Failure\"]:\n",
    "                    t_total_undecided += ct['time']\n",
    "                    res[u]['VC'][vc]['time_samples_undecided'].append (ct['time'])\n",
    "                    res[u]['VC'][vc]['time_total_undecided'] += ct['time']\n",
    "    res[u]['time'] = t_total\n",
    "    res[u]['time_undecided'] = t_total_undecided\n",
    "    res[u]['result'] = {} # TODO: summarize \n",
    "    res[u]['maxsteps'] = maxsteps\n",
    "\n",
    "t_analysis_sec = sum([ud['time'] for u,ud in res.iteritems()])\n",
    "print \"################################\"\n",
    "print \"# TOTAL CPU TIME = {0:0.1f} min\".format(t_analysis_sec/60)\n",
    "print \"################################\"\n",
    "print \"Note: divide by number of cores to get approximate wall-clock time\"\n",
    "    \n",
    "    \n",
    "if not have_checktree:\n",
    "    print \"No Check tree (needs newer GNATprove). Stopping report here.\"\n",
    "    #exit(0) #  does not kill the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# plot total time per unit\n",
    "###########################\n",
    "df_unittime = pd.DataFrame({k: v['time'] for k,v in res.iteritems() if v['time'] > 0.0}, index=['time']).T\n",
    "df_unittime.sort_values(by=['time'], inplace=True, ascending=False)\n",
    "ax=df_unittime.plot.bar(figsize=(15,5),color=[Pantone301],legend=False)\n",
    "ax.set_ylabel('analysis time [s]')\n",
    "ax.set_title('Analysis time vs. Unit')\n",
    "#ax.grid()\n",
    "plt.savefig(FOLDER + os.sep + 'units_time.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# plot maxsteps per unit\n",
    "###########################\n",
    "df_unitsteps = pd.DataFrame({k:v['maxsteps'] for k,v in res.iteritems() if v['maxsteps'] > 0}, index=['steps']).T\n",
    "df_unitsteps.sort_values(by=['steps'], inplace=True, ascending=False)\n",
    "ax=df_unitsteps.plot.bar(figsize=(15,5),color=[Pantone301],legend=False)\n",
    "ax.set_ylabel('steps')\n",
    "ax.set_title('Analysis steps vs. Unit')\n",
    "#ax.grid()\n",
    "plt.savefig(FOLDER + os.sep + 'steps.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# plot total time per VC (summarizing all units)\n",
    "##########################\n",
    "def plot_vc_overview():\n",
    "    pdata = {} # {u'UNINITIALIZED': {'cnt': 0, 'time': 0.0, 'result': {}, 'solvers': {}}, ... }\n",
    "    \n",
    "    def accumulate(vc,vcd):        \n",
    "        #print vc + \": \" + str(vcd['time_total']) + \" [\" + str(vcd['time_samples']) + \"]\"\n",
    "        if not vc in pdata: pdata[vc] = {}                   \n",
    "        pdata[vc]['time'] = pdata[vc].get('time', 0.0) + vcd['time_total']\n",
    "        pdata[vc]['time_undecided'] = pdata[vc].get('time_undecided', 0.0) + vcd['time_total_undecided']\n",
    "        if not 'samples' in pdata[vc]: pdata[vc]['samples'] = []\n",
    "        pdata[vc]['samples'].extend (vcd['time_samples'])\n",
    "        if not 'samples_undecided' in pdata[vc]: pdata[vc]['samples_undecided'] = []\n",
    "        pdata[vc]['samples_undecided'].extend (vcd['time_samples_undecided'])\n",
    "    \n",
    "    for u,ud in res.iteritems():\n",
    "        #print \"##\" + u\n",
    "        for vc,vcd in ud['VC'].iteritems():\n",
    "            accumulate(pretty_vcname(vc),vcd)\n",
    "\n",
    "    #print pdata\n",
    "            \n",
    "    # sanity check: sum of all must be approximately analysis time\n",
    "    #print \"############################\"\n",
    "    #print \"# time total for all VCs: {0:0.1f} min\".format(sum([v['time'] for k,v in pdata.iteritems()])/60)\n",
    "    #print \"############################\"\n",
    "            \n",
    "    df_vc = pd.DataFrame(pdata).T\n",
    "    df_vc.sort_values(by=['time'], inplace=True, ascending=False)\n",
    "    print df_vc.head()\n",
    "    \n",
    "    ax=df_vc.ix[:,df_vc.columns.difference(['time_samples', 'time_samples_undecided','time_undecided'])].plot.bar(logy=True,figsize=(5,2.5),color=[Pantone301,TUMred],legend=False);\n",
    "    ax.set_ylabel('Analysis time [s]')\n",
    "    ax.set_title('Total CPU time per VC type')\n",
    "    #ax.grid()\n",
    "    plt.savefig(FOLDER + os.sep + 'total_time_per_VC.pdf', bbox_inches='tight')    \n",
    "    plt.show()  \n",
    "    \n",
    "    print \"##############################\"    \n",
    "    print \"# Percentage of time spent in:\"\n",
    "    print \"##############################\"\n",
    "    for k,v in pdata.iteritems():\n",
    "        print k + \": {0:0.1f}\".format(100.0*v['time']/t_analysis_sec)\n",
    "    \n",
    "    tmp = { k: v['samples'] for k,v in pdata.iteritems()}\n",
    "    df_stats = pd.DataFrame.from_dict(tmp, orient='index').T #  we need this orient and .T because we want NaNs to fill unequal lengths\n",
    "    #print df_stats.head()\n",
    "    #print df_stats.describe()    \n",
    "    exclude_columns = ['Fp Overflow Check','Assert'] # because Float is extremely slow\n",
    "    ax=df_stats.ix[:,df_stats.columns.difference(exclude_columns)].plot.box(vert=False,figsize=(20,6),showfliers=False);\n",
    "    #plt.setp( ax.xaxis.get_majorticklabels(), rotation=90 )\n",
    "    ax.set_ylabel('Analysis time [s]')\n",
    "    ax.set_title('Statistical analysis time of a single VC')\n",
    "    #ax.grid()\n",
    "    plt.savefig(FOLDER + os.sep + 'statistical_time_per_VC.pdf', bbox_inches='tight')    \n",
    "    plt.show()  \n",
    "        \n",
    "        \n",
    "plot_vc_overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_vc_time_per_unit(vcname):\n",
    "    \"\"\"\n",
    "    Make a plot showing time spent in a specific VC for all modules\n",
    "    \"\"\"\n",
    "    pdata = {}\n",
    "    for u,ud in res.iteritems():\n",
    "        if vcname in ud['VC']:\n",
    "            t = ud['VC'][vcname]['time_total']\n",
    "            t2 = ud['VC'][vcname]['time_total_undecided']\n",
    "            if t > 0.0: \n",
    "                pdata[u] = { 'time total': t, 'time undecided':t2 }\n",
    "                \n",
    "\n",
    "    if len(pdata.keys()) == 0: return\n",
    "                \n",
    "    df_vc = pd.DataFrame(pdata).T\n",
    "    df_vc.sort_values(by=['time total'], inplace=True, ascending=False)\n",
    "    ax=df_vc.plot.bar(figsize=(13,7),color=[Pantone301, TUMred],legend=True);\n",
    "    ax.set_ylabel('Analysis time [s]')\n",
    "    ax.set_title('Analysis time for ' + pretty_vcname(vcname))\n",
    "    #ax.grid()\n",
    "    plt.savefig(FOLDER + os.sep + 'time_in_' + vcname + '.pdf', bbox_inches='tight')    \n",
    "    plt.show()    \n",
    "    \n",
    "\n",
    "\n",
    "#########################################\n",
    "# plot VC time of a specific VC per unit\n",
    "#########################################\n",
    "\n",
    "for vc in totals[\"rules\"].keys():    \n",
    "    plot_vc_time_per_unit(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How often do we fail because of timeout/stepout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print \" \"\n",
    "#print \"Mission:\"\n",
    "#print res['mission']\n",
    "#  u'VC_RANGE_CHECK': {'cnt': 61, 'time': 68.78999999999995, 'result': {'steplimit': 3, u'Valid': 48, 'undefined': 10}, 'solvers': {u'CVC4': 51, u'altergo': 3, u'CVC4_CE': 3, u'Z3': 3}}\n",
    "\n",
    "def plot_failure_reason(unit):\n",
    "    \"\"\"\n",
    "    Make a plot showing why checks in unit fail\n",
    "    \"\"\"            \n",
    "    pdata = {v : vd['result'] for v,vd in res[unit]['VC'].iteritems() if vd and 'cnt' in vd and vd['cnt'] > 0}\n",
    "    \n",
    "    #print unit + \":\" + str(pdata)\n",
    "    num_fails = sum([1 for v,vd in pdata.iteritems() if not 'Valid' in vd.keys() or len(vd.keys()) > 1])\n",
    "    if num_fails == 0:\n",
    "        print \" \"\n",
    "        print \"no fails for \" + unit\n",
    "        return\n",
    "    \n",
    "    # add 'cnt' to each VC (total)\n",
    "    for v,vd in pdata.iteritems():        \n",
    "        vd['cnt'] = res[unit]['VC'][v]['cnt']            \n",
    "        \n",
    "    \n",
    "    \n",
    "    if len(pdata.keys()) == 0: \n",
    "        print \"no data for \" + unit\n",
    "        return\n",
    "          \n",
    "    df_vc = pd.DataFrame(pdata).T\n",
    "    df_vc.sort_values(by=['cnt'], inplace=True, ascending=False)\n",
    "    \n",
    "    exclude_columns=['cnt']\n",
    "    ax=df_vc.ix[:,df_vc.columns.difference(exclude_columns)].plot.bar(figsize=(10,5));\n",
    "    ax.set_ylabel('Number of')\n",
    "    ax.set_title('Why checks fail in ' + unit)\n",
    "    #ax.grid()\n",
    "    plt.savefig(FOLDER + os.sep + 'unit_fails_' + unit + '.pdf', bbox_inches='tight')    \n",
    "    plt.show() \n",
    "    \n",
    "for k in units.keys():\n",
    "    if units[k][\"success\"] < 100.0: plot_failure_reason (k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
